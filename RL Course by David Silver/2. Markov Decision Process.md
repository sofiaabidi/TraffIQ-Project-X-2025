## Lecture-2: Markov Decision Process (MDP)

1. Introduction to MDPs:
- MDP describes an env for RL. env is fully observable. current state characterizes the process.
optimal control primarily deal with cont. MDPs.
pomdp can be converted to MDPs. bandits are MDP with single state (refer K-Arm bandit prob)
- State transition matrix: defines transition probabilities from **all** states ‘s’ to all successor states s’.
summation of each row of matrix=1
- Memoryless property: in p&s context, future beahviour of a RV is independent of past ones.
- Markov Process: memoryless random process (seq of random states with markov prop)
(refer the markov chain example in the video, try to find some on your own)

2. Markov Reward Process:
- MRP is markov chain with values
tuple with <S, P, R, *γ*>. state, transition prob matrix, reward funcn, discount factor.
S is a finite set of states. R is reward funcn (our goal is to max. R). γ belongs to [0,1].
- Return: return Gt is total discounted reward from time-step t. to make return finite we use discount factor, return is the summation from 0 to infinite of each reward with a co-factor of *γ^n-1*  where n is the nth reward state. disc factor close to 0 = near evaluation and vice-versa.
- most of the MDPs are discounted to avoid infinite markov processes, get finite values.
undiscounted MDPs ie γ = 1 if the sequences terminate at a point. immediate rewards preferred.
- Value Function: v(s) us the long term value of state s. expected return from from starting state.
personal note: try creating some random MRP and play around with the return functions with experimentation of *γ.*

3. Bellman Equation for MRPs:
- value function can be decomposed in 2 parts:
immediate reward. discounted value of successor state γv(St+1)
- v(s) = E[Gt | St = s]
v(s) = E[Rt+1 + γ v(St+1) | St =s]
- bellman eqn in matrix: v is a column vector with one entry per state
- cubic complexity: O(n^3) for n states. bellman eqn is a linear eqn. (can be solved directly)
v = (1 - gamma.P)^-1 * R. (direct soln for small MRPs)
many iterative methods for large MRPs: DP, Monte-Carlo evaluation, Temporal difference.
personal note: up until this point look up briefly for these methods and correlate with proj.

4. Markov Decision Process: 
- MDP is a mrp with a decision factor(action). an env in which all states are markov.
MDP a tuple with: added action parameter to MRP. (A)
A is a finite set of actions.
- Policies: policy π is a distribution over actions GIVEN states. π(a|s) = P[At=a | St=s].
given a MDP and a policy π, the state sequence S1,S2.. is a markov process <S,P^π >
state and reward sequence s1,r2,s2.. is a markov reward process. (check for P^π eqn)
- Value Function: i. action-value function: Qπ(s,a) is the expected return starting from state s, taking action a and then following policy π. 
state-value function: Vπ (s) of an MDP is expected return starting from state s and then following policy π.
- Bellman Expectation Eqn (for state-value function): 
decompose the state value funcn into immediate reward + discounted value of successor state.
similarly the action value function can be decomposed.

vπ(s) = Eπ [Rt+1 + γvπ(St+1) | St = s]

qπ(s, a) = Eπ [Rt+1 + γqπ(St+1, At+1) | St = s, At = a]

- (refer bellman expectation eqn for qπ from the slides and the eqn in matrix form)
- Optimal Value Function: optimal state-value funcn v*(s) is max value funcn over all policies.
optimal action-value function q*(s,a) is the maximum action-value funcn over all policies.
- Optimal Policy: an optimal policy is compared by comparing its respective state-value funcn with another policy’s.
thm: for any MDP: there exists an optimal policy π* ie. ≥ all other policies. π* ≥ π, for all π.
all optimal policies achieve the optimal value function. if they are optimal then they achieve the same optimal action-value funcn q*(s,a)
- To find an optimal policy: recursively related to bellman optimality eqns. instead of taking avg of all q-values just take the one which is the max hence giving you the max value funcn.
similar idea for other optimality eqns. refer the slides if ever needed.
- solve the bellman optimality eqn: its a non linear eqn thus no closed form soln
many iterative soln methods include: value-iteration, policy-iteration, q-learning, sarsa.