# Untitled

## Lecture-5: Model Free Control

- model free prediction is estimate the value funcn of an unknown MDP using mc-evaluation or td-learning. 
goal of model-free control: optimise value function of an unknown MDP.

1. On Policy Monte-Carlo Control:
- problems which can be modelled as MDP: portfolio management, helicopter, parallel parking etc.
for these problems: either the mdp is unkown but experience can be sampled
or if the mdp is known but its too big to use except by samples. use model free control to solve it
- On-Policy: learn ON the job. learn abt policy π from experienced sampled from π.
Off-Policy: look OVER someone’s shoulder. learn abt policy π from experience sampled from mu.
target policy is our set policy (epsilon greedy) but in behaviour policy we just use greedy policy to select action without even considering our target policy.
- Generalised Policy Iteration: policy evaluation: estimate Vπ (iterative policy evaluation)
policy improvement: generate π’ ≥ π (greedy policy improvement)
- Model-Free policy iteration using action-value funcn: greedy policy improvement over state value-funcn V(s) requires model of mdp. greedy policy improvement over action-value funcn Q(s,a) is model free.
- Epsilon-Greedy Exploration: all ‘m’ actions tried with non-zero prob.
with prob. 1-epsilon, choose greedy action. with prob epsilon choose any action at random.
- Epsilon-Greedy Policy Improvement: for any epsilon-greedy policy π, the policy π’ wrt qπ is an improvement. vπ’(s) ≥ vπ(s)
- Monte-Carlo Policy Iteration: evaluate a policy by experimenting with action-value funcn. Q=qπ
for policy improvement, use epsilon-greedy improvement.
- Monte-Carlo Improvement: need not complete an entire episode, just use an estimate with a few actions, idea is run 1 episode improve the policy. Q is an approximation of qπ, balances time.
- How do we actually reach to the best possible policy:- GLIE: 
run an episode and immediately improve your policy (Q-Value)
Greedy in the Limit with Infinite Exploration (GLIE). meet 2 conditions:-
i. all state-action pairs are explored infinitely many times to explore each and every action in the state space. 
ii. eventually we need to use the most greedy policy to satisfy the bellman optimality eqn. eventually decay the value to 0 to reach an argmax value.
- GLIE Monte-Carlo Control: sample kth episode using our policy π: s,a,r..
Improve policy based on new action-value function: epsilon:- 1/k. π: epsilon-greedy(Q)
refer the monte-carlo control in blackjack example in the vid.

1. On-Policy Temporal-Difference Learning
- MC vs TD Control:
TD learning advantages: lower variance, online, incomplete sequences.
natural idea: use TD instead of MC in control loop. apply TD to Q(s,a) to update every time-step.
- SARSA: start with a state S and an action A, we get a reward R based on which we land in a state S’ and decide our next action A’.
- On-Policy control with SARSA: for every time-step continuous update the q values using sarsa.
SARSA Algorithm for on-policy control:-
arbitrarily initialize a lookup table for Q-Value, repeat for each episode, initialize S, choose A from S using derived policy from Q, take action A, observe R, S’. choose A’ from S’ using policy derived from Q and repeat for each step of episode until S is terminated.
this is an on-policy algo i.e. we are selecting actions which demonstrate the policy.
- Convergence of SARSA: sarsa converges to optimal action-value funcn under GLIE sequences of policies and sequence of step sizes alpha-t, eventually the difference in newer q-values is small.
windy-gridworld example with standard moveset.
- n-step SARSA: it considers N state-action pairs for updating the Q-values instead of just looking only at the immediate next state-action pair to update Q-value, looks up N steps ahead in trajectory to calculate a more comprehensive return.
- forward view sarsa(λ): balances bridge betn MC and TD learning using eligibility traces.
eligibility traces: temporary record of occurrence of an event like visiting a state/taking an action
intuitive overview: λ=0: only 1 step return is used. TD(0)
λ=1: approaches full return over episode: mc case. here we consider 0 < λ < 1
forward view is theoretical requiring future reward and actions. (not practical in online RL, hence establishing backward-view sarsa(λ))
 q^λ return combines all n-step q returns including weights of (1-λ)λ^n-1
- backward view sarsa(λ): one eligibility trace for each state-action pair, Eo(s,a) = 0 and Et eqn.
- sarsa(λ) algorithm: refer the grid example in the video and if possible go through it again.

1. Off-Policy Learning:
- evaluate target policy pi to compute vpi or qpi while following behaviour policy mu
eg: learn from observing other agents/humans. reuse exp from old policies. learn optimal policy while following exploratory policy. learn multiple policies while following one policy.
- Importance sampling: estimate expectation of a different distribution. 
for off-policy monte carlo: use returns generated from mu to evaluate pi, weight return Gt according to similarity between policies. multiply importance sampling correction along whole episode and update value towards corrected return. CANT use if mu=0 but pi≠0, this drastically increases variance hence off-policy isnt possible for mc-methods.
- Importance Sampling for off-policy TD: 
use TD targets generated from mu to evaluate pi, weigh TD target by imp. sampling
only need a single importance sampling correction. 
much lower variance than MC-methods.
- Q-Learning: no importance sampling req. off-policy learning of action values Q(s,a)
next action is chosen using behaviour policy At+1 BUT we consider alternative successor action A’ and update Q(St,At) towards value of alternative action.
- allow both behaviour and target policies to improve.
target policy Pi is greedy wrt Q(s,a)
behaviour policy Mu is epsilon-greedy wrt Q(s,a) to allow slight exploration
Q-learning target then simplifies = Rt+1 + max.gamma.Q(St+1, a’)\
- q-learning control converges to optimal action-value function.
- Relationship betn DP and TD:
bellman expectation eqn for Vpi(s): iterative policy evaluation in full-backup (DP) or TD learning in simple backup (TD)
bellman expectation eqn for qpi(s,a): Q-policy iteration and SARSA resp.
bellman expectation eqn for for q*(s,a): Q-Value iteration and Q-Learning resp.